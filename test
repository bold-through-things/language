#!/usr/bin/env python3

from dataclasses import dataclass
import fnmatch
import json
import sys
import threading
import time
import unittest
import subprocess
import tempfile
import os
from pathlib import Path

import unittest
import subprocess
import tempfile
import shutil
import os
from pathlib import Path
from typing import Optional, Tuple
from subprocess import Popen, PIPE
from typing import List

from test_diff_reporter import create_test_diff_reporter
from unit_test_discovery import discover_unit_tests

TEST_ROOT = Path("tests")
EXECUTABLE = "out.js"

def find_deno_executable() -> Path:
    """Find Deno executable, checking local installation first, then global."""
    # Check local installation first
    local_deno = Path(".deno/bin/deno")
    if local_deno.is_file():
        return local_deno
    
    # Check global installation via which
    import shutil
    global_deno = shutil.which("deno")
    if global_deno:
        return Path(global_deno)
    
    # Return local path as fallback (for error messaging)
    return local_deno

DENO_PATH = find_deno_executable()

def read_file(path: Path) -> Optional[str]:
    return path.read_text(encoding="utf-8") if path.exists() else None
def run_with_input(exec_cmd: List[str], stdin: str, line_delayed: bool = True, cwd: Path = None) -> Tuple[int, str, str]:
    proc = Popen(exec_cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, text=True, bufsize=1, cwd=cwd)

    output_lines: List[str] = []
    error_lines: List[str] = []

    def read_output() -> None:
        for line in proc.stdout:
            output_lines.append(line)

    def read_error() -> None:
        for line in proc.stderr:
            error_lines.append(line)

    threading.Thread(target=read_output, daemon=True).start()
    threading.Thread(target=read_error, daemon=True).start()

    if proc.stdin and stdin is not None:
        if line_delayed:
            for line in stdin.splitlines():
                time.sleep(0.6)
                proc.stdin.write(line + "\n")
                proc.stdin.flush()
        else:
            proc.stdin.write(stdin)
            proc.stdin.flush()
    
    if proc.stdin:
        proc.stdin.close()

    proc.wait()

    if proc.stdout:
        proc.stdout.close()
    if proc.stderr:
        proc.stderr.close()

    return proc.returncode, "".join(output_lines), "".join(error_lines)

@dataclass(kw_only=True)
class TestCase:
    name: str = ""
    def_path: Path
    case_path: Path
    code_path: Path

import json
from pathlib import Path
from typing import TypeAlias
from itertools import product

def discover_tests(args) -> list[TestCase]:
    def resolve(pattern: str, base: Path) -> list[Path]:
        if not pattern:
            raise ValueError(f"Empty pattern in {base}")
        return list(base.glob(pattern)) if "*" in pattern else [base / pattern]

    import re

    glob = lambda x: True
    if args.glob:
        pattern = fnmatch.translate(f"**{args.glob}**") # TODO - might fail with --glob anagra*
        glob = re.compile(pattern)
        glob = glob.match

    tests: list[TestCase] = []

    for tests_json_path in TEST_ROOT.rglob("tests.json"):

        with open(tests_json_path, "r") as f:
            # TODO - instead of JSON leverage our own format
            entries = json.load(f)

        base_dir = tests_json_path.parent

        for entry in entries:
            code_glob = entry["code"]
            case_glob = entry["case"]

            code_paths = resolve(code_glob, base_dir)
            case_paths = resolve(case_glob, base_dir)

            for code_path, case_path in product(code_paths, case_paths):
                def_parts = base_dir.relative_to(TEST_ROOT).parts
                code_parts = code_path.relative_to(base_dir).parts
                case_parts = case_path.relative_to(base_dir).parts
                name = "test_" + "_".join(def_parts + code_parts + case_parts)
                if not glob(str(name)):
                    print(f"ignoring `{name}` per midglob `{args.glob}`")
                    continue
                tests.append(TestCase(def_path=base_dir, case_path=case_path, code_path=code_path, name=name))

    return tests

def validate_gitignore_for_test(test_case, tc: TestCase):
    """
    validate that gitignore properly handles test artifacts for this specific test
    only validates test execution output (JS emission, macro expansion, actual output)
    """
    from test_discovery import get_test_artifacts
    
    # get all artifacts that should be tracked for this test
    artifacts = get_test_artifacts(tc)
    
    if not artifacts:
        return  # no artifacts to validate
    
    # check that valuable artifacts are properly tracked by git
    for artifact in artifacts:
        if artifact.exists():
            returncode, stdout, stderr = run_with_input(
                ["git", "ls-files", str(artifact)], 
                stdin="", 
                line_delayed=False
            )
            test_case.assertEqual(returncode, 0, f"git ls-files failed for {artifact}: {stderr}")
            test_case.assertTrue(stdout.strip(), f"must track valuable artifact: {artifact}")
    
    # check that test_diffs directory is ignored if it exists
    test_diffs_dirs = [
        tc.case_path / "test_diffs",
        tc.code_path / "test_diffs"
    ]
    
    for test_diffs_dir in test_diffs_dirs:
        if test_diffs_dir.exists():
            returncode, stdout, stderr = run_with_input(
                ["git", "check-ignore", str(test_diffs_dir)], 
                stdin="", 
                line_delayed=False
            )
            test_case.assertEqual(returncode, 0, f"must ignore test_diffs directory: {test_diffs_dir}")

def make_test_method(tc: TestCase, args):
    def test(self: unittest.TestCase) -> None:
        case_dir = tc.case_path
        code_dir = tc.code_path
        print(tc.name)
        print(f"running test for {tc}")
        
        # create improved diff reporter
        diff_reporter = create_test_diff_reporter(tc)
        
        expected_compile_err = read_file(code_dir / "compile.stderr.expected")
        expected_runtime_err = read_file(case_dir / "runtime.stderr.expected")
        expected_stdout = read_file(case_dir / "success.stdout.expected")
        if args.stdin:
            stdin_text = sys.stdin.read()
        else:
            stdin_text = read_file(case_dir / "stdin")

        with tempfile.TemporaryDirectory() as tmpdir_str:
            tmpdir = Path(tmpdir_str)
            compile_err_actual = code_dir / "compile.stderr.actual"
            if compile_err_actual.exists():
                os.remove(compile_err_actual)
            out_path = code_dir / EXECUTABLE

            # TODO - is this obsolete? what are we gaining from this copy ?
            shutil.copytree(code_dir, tmpdir, dirs_exist_ok=True)
            shutil.copytree(case_dir, tmpdir, dirs_exist_ok=True)

            if args.compile:
                compiler_path = Path("compiler/src/main.py")
                
                if args.expand:
                    # Two-step compilation: .67lang → .67lang.expanded → .js
                    print(f"{case_dir}: two-step compilation (expand mode)...")
                    
                    # Step 1: .67lang → .67lang.expanded
                    expanded_path = code_dir / ".67lang.expanded"
                    step1_cmd = [compiler_path.absolute(), code_dir.absolute(), expanded_path.absolute(), "--errors-file", compile_err_actual.absolute(), "--expand"]
                    
                    # Add any additional compiler arguments to step 1
                    if args.compiler_args:
                        step1_cmd.extend(args.compiler_args)
                    
                    print(f"{case_dir}: step 1 - expanding...")
                    step1_proc = subprocess.run(step1_cmd, cwd=tmpdir, capture_output=True, text=True)
                    print(f"{case_dir}: step 1 complete. {step1_proc.returncode=}")
                    print(step1_proc.stdout)
                    print(step1_proc.stderr)
                    
                    if step1_proc.returncode != 0:
                        if expected_compile_err is not None:
                            actual_compile_err = read_file(compile_err_actual) or ""
                            is_equal, error_msg = diff_reporter.compare_text(
                                actual_compile_err, expected_compile_err, "compile_stderr"
                            )
                            self.assertTrue(is_equal, msg=error_msg or "Compile stderr mismatch")
                            self.assertNotEqual(step1_proc.returncode, 0, msg="Expected compile to fail")
                            return
                        self.assertEqual(step1_proc.returncode, 0, msg=f"Step 1 (expand) failed unexpectedly\n{step1_proc.stderr}")
                        return
                    
                    
                    step2_cmd = [compiler_path.absolute(), code_dir.absolute(), out_path.absolute(), "--errors-file", compile_err_actual.absolute(), "--rte"]
                    
                    # Add any additional compiler arguments to step 2
                    if args.compiler_args:
                        step2_cmd.extend(args.compiler_args)
                    
                    print(f"{case_dir}: step 2 - compiling expanded form to JS...")
                    compile_proc = subprocess.run(step2_cmd, cwd=tmpdir, capture_output=True, text=True)
                    print(f"{case_dir}: step 2 complete. {compile_proc.returncode=}")
                    print(compile_proc.stdout)
                    print(compile_proc.stderr)
                    
                else:
                    # Single-step compilation: .67lang → .js
                    print(f"{case_dir}: single-step compilation...")
                    compile_cmd = [compiler_path.absolute(), tmpdir.absolute(), out_path.absolute(), "--errors-file", compile_err_actual.absolute()]
                    
                    # Add any additional compiler arguments
                    if args.compiler_args:
                        compile_cmd.extend(args.compiler_args)
                    
                    compile_proc = subprocess.run(compile_cmd, cwd=tmpdir, capture_output=True, text=True)
                    print(f"{case_dir}: done compiling. {compile_proc.returncode=}")
                    print(compile_proc.stdout)
                    print(compile_proc.stderr)

                if expected_compile_err is not None:
                    actual_compile_err = read_file(compile_err_actual) or ""
                    is_equal, error_msg = diff_reporter.compare_text(
                        actual_compile_err, expected_compile_err, "compile_stderr"
                    )
                    self.assertTrue(is_equal, msg=error_msg or "Compile stderr mismatch")
                    self.assertNotEqual(compile_proc.returncode, 0, msg="Expected compile to fail")
                    return

                self.assertEqual(compile_proc.returncode, 0, msg=f"Compile failed unexpectedly\n{compile_proc.stderr}")

            if args.run:
                fuck_you_python = lambda arg, if_cond: arg if if_cond else None
                
                exec_cmd = [DENO_PATH.absolute(), "run", "--allow-read", fuck_you_python("--inspect-brk=127.0.0.1:9229", args.debug), out_path.absolute()]
                exec_cmd = list(filter(None, exec_cmd))
                print(f"{case_dir}: running... {exec_cmd}")
                returncode, stdout, stderr = run_with_input(exec_cmd, stdin=stdin_text, line_delayed=False, cwd=case_dir)
                print(f"{case_dir}: complete.")
                print(stdout)

                if args.stdin:
                    pass # custom input? don't assert.
                else:
                    # print(stdout.strip())
                    # print(stderr.strip())
                    if expected_runtime_err is not None:
                        is_equal, error_msg = diff_reporter.compare_text(
                            stderr.strip(), expected_runtime_err.strip(), "runtime_stderr"
                        )
                        self.assertTrue(is_equal, msg=error_msg or "Runtime stderr mismatch")
                        self.assertNotEqual(returncode, 0, msg="Expected runtime failure")
                    else:
                        expected_out = expected_stdout.strip() if expected_stdout else ""
                        # Substitute test suite placeholders
                        script_dir = Path(__file__).parent.absolute()
                        expected_out = expected_out.replace("67lang:test_suite:project_path", str(script_dir))
                        is_equal, error_msg = diff_reporter.compare_text(
                            stdout.strip(), expected_out, "runtime_stdout"
                        )
                        self.assertTrue(is_equal, msg=error_msg or "Runtime stdout mismatch")
                        self.assertEqual(returncode, 0, msg="Runtime failed unexpectedly")
        
        # Validate gitignore behavior for test artifacts  
        validate_gitignore_for_test(self, tc)
    return test

if __name__ == "__main__":    
    print("67.8% OUR GREATEST ENEMY IS OUR INDIFFERENCE")
    
    # Dynamically create test methods
    class MyLangTestCase(unittest.TestCase):
        def test_silent_compilation(self):
            """Test that compilation is silent by default using existing test case"""
            
            # Use an existing working test case
            test_dir = Path(__file__).parent / "tests" / "basics" / "anagram_groups"
            
            with tempfile.TemporaryDirectory() as tmpdir:
                tmpdir_path = Path(tmpdir)
                output_file = tmpdir_path / "out.js"
                
                # Run compiler without --log
                compiler_path = Path("compiler") / "src" / "main.py"
                result = subprocess.run(
                    ["python3", str(compiler_path), str(test_dir), str(output_file)],
                    capture_output=True,
                    text=True,
                    cwd=Path(__file__).parent
                )
                
                # Check that compilation succeeded
                self.assertEqual(result.returncode, 0, f"Compilation failed: {result.stderr}")
                
                # Check that stdout contains the final flicker message and stderr is empty
                stdout_lines = [line.strip() for line in result.stdout.strip().split('\n') if line.strip()]
                stderr_lines = [line.strip() for line in result.stderr.strip().split('\n') if line.strip()]
                expected_message = "refactor confidently when the flame flickers."
                
                # The flicker message should be the only output, either in stdout or stderr
                total_lines = stdout_lines + stderr_lines
                self.assertEqual(len(total_lines), 1, f"Expected only the flicker message, but got stdout: {stdout_lines}, stderr: {stderr_lines}")
                self.assertEqual(total_lines[0], expected_message, f"Expected '{expected_message}', got '{total_lines[0]}'")
                
                # Verify that output file was created
                self.assertTrue(output_file.exists(), "Output file was not created")

        def test_verbose_compilation(self):
            """Test that compilation shows logs when --log is provided"""
            
            # Use an existing working test case
            test_dir = Path(__file__).parent / "tests" / "basics" / "anagram_groups"
            
            with tempfile.TemporaryDirectory() as tmpdir:
                tmpdir_path = Path(tmpdir)
                output_file = tmpdir_path / "out.js"
                
                # Run compiler with --log registry
                compiler_path = Path("compiler") / "src" / "main.py"
                result = subprocess.run(
                    ["python3", str(compiler_path), str(test_dir), str(output_file), "--log", "registry"],
                    capture_output=True,
                    text=True,
                    cwd=Path(__file__).parent
                )
                
                # Check that compilation succeeded
                self.assertEqual(result.returncode, 0, f"Compilation failed: {result.stderr}")
                
                # Check that stderr contains registry log messages
                all_output = result.stdout + result.stderr
                self.assertIn("[registry] registering macro", all_output, "Expected registry log messages when --log registry is used")
                self.assertIn("refactor confidently when the flame flickers.", all_output, "Expected final flicker message")

    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("-g", "--glob", help="only run tests matching this midglob. try `--glob anagra` or `--glob ypeche` or `--glob order*main`!")
    parser.add_argument("-s", "--stdin", action='store_true', help="pass the Python stdin to each test. sometimes useful for manual debugging of runtime execution")
    parser.add_argument("-c", "--compile", action='store_true', help="only check compilation of target tests")
    parser.add_argument("-d", "--debug", action='store_true', help="execute runtime in debug mode")
    parser.add_argument("-r", "--run", action='store_true', help="skip compilation. assume the tests are already compiled, and only run the existing output")
    parser.add_argument("--expand", action='store_true', help="test two-step compilation: `.67lang → .67lang.expanded → .js` WIP and expected to fail currently!")
    parser.add_argument("--unit", action='store_true', help="run only unit tests")
    parser.add_argument("--e2e", action='store_true', help="run only end-to-end tests")

    # Parse known args first, then treat everything after -- as compiler args
    if "--" in sys.argv:
        dash_index = sys.argv.index("--")
        test_args = sys.argv[1:dash_index]
        compiler_args = sys.argv[dash_index + 1:]
        sys.argv = [sys.argv[0]] + test_args  # Set up for argparse
    else:
        compiler_args = []

    args, remaining = parser.parse_known_args()
    sys.argv = [sys.argv[0]] + remaining  # leave only unknown args for unittest
    
    # Store compiler args on the args object
    args.compiler_args = compiler_args

    # Determine which test types to run
    run_e2e = True
    run_unit = True
    
    if args.unit and not args.e2e:
        run_e2e = False
        run_unit = True
    elif args.e2e and not args.unit:
        run_e2e = True
        run_unit = False
    # If both or neither specified, run both
    
    if not args.compile and not args.run:
        # default behavior
        args.run = True
        args.compile = True

    if args.run:
        if not DENO_PATH.is_file():
            raise ValueError(
                "run with what? Deno pls.\n"
                "corrupt your innocent OS:\n curl -fsSL https://deno.land/install.sh | sh\n"
                "contain the scawy dinosaurs into this workspace:\n curl -fsSL https://deno.land/install.sh | DENO_INSTALL=.deno sh"
            )
    
    # Add discovered e2e tests
    if run_e2e:
        for tc in discover_tests(args):
            setattr(MyLangTestCase, tc.name, make_test_method(tc, args))
    
    # Add discovered unit tests
    if run_unit:
        unit_tests = discover_unit_tests()
        for test_name, test_class, test_method_name in unit_tests:
            # Create a method that runs the specific test method
            
            def make_unit_test_method(test_cls, method_name):
                def unit_test(self):
                    # Create an instance of the test class and run the method
                    test_instance = test_cls()
                    test_method = getattr(test_instance, method_name)
                    test_method()
                return unit_test
            
            method = make_unit_test_method(test_class, test_method_name)
            method._is_unit_test = True
            # Ensure unittest can find it by prefixing with test_
            unittest_name = f"test_{test_name}" if not test_name.startswith('test_') else test_name
            setattr(MyLangTestCase, unittest_name, method)
    
    # Apply glob filter to ALL tests at the end
    import re
    glob_filter = lambda x: True
    if args.glob:
        pattern = fnmatch.translate(f"**{args.glob}**")
        glob_filter = re.compile(pattern).match
    
    # Filter all test methods based on glob and test type flags
    all_test_methods = [name for name in dir(MyLangTestCase) if name.startswith('test_') or name.startswith('unit_')]
    for test_method_name in all_test_methods:
        should_keep = True
        
        # Check test type filtering
        test_method = getattr(MyLangTestCase, test_method_name)
        is_unit_test = hasattr(test_method, '_is_unit_test')
        
        if is_unit_test:
            if not run_unit:
                should_keep = False
                print(f"ignoring `{test_method_name}` (unit test, running e2e only)")
        else:
            # It's an e2e test (including silent/verbose compilation tests)
            if not run_e2e:
                should_keep = False
                print(f"ignoring `{test_method_name}` (e2e test, running unit only)")
        
        # Check glob filtering
        if should_keep and not glob_filter(test_method_name):
            should_keep = False
            print(f"ignoring `{test_method_name}` per midglob `{args.glob}`")
        
        # Remove test if it shouldn't be kept
        if not should_keep:
            delattr(MyLangTestCase, test_method_name)
    
    if __name__ == "__main__":
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromTestCase(MyLangTestCase)
        runner = unittest.TextTestRunner()
        runner.run(suite)

